# Create 6 subnets. 3 public 3 private. Private should not have igw attached as a route.
# only NAT gateway. Create SG for accessing NG (ssh access) and keypair.
# eksctl create cluster --config-file ./this-file --profile=aws_profile
apiVersion: eksctl.io/v1alpha5
cloudWatch:
  clusterLogging: {}

kind: ClusterConfig
kubernetesNetworkConfig:
  ipFamily: IPv4
managedNodeGroups:
- amiFamily: Ubuntu2004
  desiredCapacity: 3
  disableIMDSv1: false
  disablePodIMDS: false
  iam:
    withAddonPolicies:
      albIngress: true
      appMesh: false
      appMeshPreview: false
      autoScaler: true
      certManager: false
      cloudWatch: false
      ebs: false
      efs: false
      externalDNS: false
      fsx: false
      imageBuilder: true
      xRay: false
    attachPolicyARNs:
      - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
      - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      # First three are mandatory. The 4th one can be changed.
      - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPowerUser
  instancePrefix: k8s-eksctl1
  instanceSelector: {}
  instanceType: t3.xlarge
  labels:
    alpha.eksctl.io/cluster-name: k8s-eksctl1
    alpha.eksctl.io/nodegroup-name: ng-1
  maxSize: 6
  minSize: 3
  name: ng-1
  privateNetworking: false
  releaseVersion: ""
  securityGroups:
    withLocal: null
    withShared: null
  ssh:
    allow: true
    publicKeyName: <your-key-here>
    sourceSecurityGroupIds:
      - <your-SG-here>
  tags:
    alpha.eksctl.io/nodegroup-name: ng-1
    alpha.eksctl.io/nodegroup-type: managed
    k8s.io/cluster-autoscaler/enabled: "true"
    k8s.io/cluster-autoscaler/cluster-13: "owned"
  volumeIOPS: 3000
  volumeSize: 400
  volumeThroughput: 125
  volumeType: gp3
metadata:
  name: k8s-gws1
  region: eu-west-1
  version: "1.21"
privateCluster:
  enabled: false
  skipEndpointCreation: false
vpc:
  id: "<your-VPC-id-here>"  # (optional, must match VPC ID used for each subnet below)
  cidr: "your-CIRD-here"       # (optional, must match CIDR used by the given VPC)
  subnets:
    # Must provide 'private' and/or 'public' subnets by availibility zone as shown.
    # And they must exist. Otherwise eksctl will create his own vpc and subnets
    # https://eksctl.io/usage/vpc-networking/
    public:
      eu-west-1a:
        id: "<your-subent-id-one-here>"
      eu-west-1b:
        id: "<your-subent-id-two-here>"
      eu-west-1c:
        id: "<your-subent-id-three-here>"
  # The next option "publicAccessCIDRs" is about tightening security. We will restrict access to k8s API from an IP address.
  # This means if somebody steals your k8s credentials, he will also need to login to your network or VPN to be able to do
  # something. If you enable publicAccessCIDRs option, then you also need to enable clusterEndpoints "privateAccess" and
  # "publicAccess" both to true. Otherwise you will get an error like: "Instances failed to join the kubernetes cluster"
  # More about it here: https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html

  clusterEndpoints:
    privateAccess: true
    publicAccess: true
  publicAccessCIDRs: ["your.ip.addr.here/32", "your.net.range.here/24"]

# For Fargate profiles, you need to have subnets that doesn't route to IGW. Otherwise the build
# will fail. So create three new route tables and add route 0.0.0.0/0 that points to NAT GW.
# And then create/associate subnets to each of the route table.
fargateProfiles:
  - name: fp-profile1
    selectors:
      - namespace: fp-dev
      - namespace: fp-stg
      - namespace: fp-prod
    subnets:
      - <your-private-subent-id-one-here>
      - <your-private-subent-id-two-here>
      - <your-private-subent-id-three-here>
iam:
  vpcResourceControllerPolicy: true
  withOIDC: true
  serviceAccounts:
  - metadata:
      name: aws-load-balancer-controller
      namespace: kube-system
    wellKnownPolicies:
      awsLoadBalancerController: true
  - metadata:
      name: ebs-csi-controller-sa
      namespace: kube-system
    wellKnownPolicies:
      ebsCSIController: true
  - metadata:
      name: efs-csi-controller-sa
      namespace: kube-system
    wellKnownPolicies:
      efsCSIController: true
  - metadata:
      name: external-dns
      namespace: kube-system
    wellKnownPolicies:
      externalDNS: true
  - metadata:
      name: cert-manager
      namespace: cert-manager
    wellKnownPolicies:
      certManager: true
  - metadata:
      name: cluster-autoscaler
      namespace: kube-system
      labels: {aws-usage: "cluster-ops"}
    wellKnownPolicies:
      autoScaler: true
  - metadata:
      name: build-service
      namespace: ci-cd
    wellKnownPolicies:
      imageBuilder: true
  - metadata:
      name: cache-access
      namespace: backend-apps
      labels: {aws-usage: "application"}
    attachPolicyARNs:
    - "arn:aws:iam::aws:policy/AmazonDynamoDBReadOnlyAccess"
    - "arn:aws:iam::aws:policy/AmazonElastiCacheFullAccess"
  - metadata:
      name: autoscaler-service
      namespace: kube-system
    attachPolicy: # inline policy can be defined along with `attachPolicyARNs`
      Version: "2012-10-17"
      Statement:
      - Effect: Allow
        Action:
        - "autoscaling:DescribeAutoScalingGroups"
        - "autoscaling:DescribeAutoScalingInstances"
        - "autoscaling:DescribeLaunchConfigurations"
        - "autoscaling:DescribeTags"
        - "autoscaling:SetDesiredCapacity"
        - "autoscaling:TerminateInstanceInAutoScalingGroup"
        - "ec2:DescribeLaunchTemplateVersions"
        Resource: '*'
